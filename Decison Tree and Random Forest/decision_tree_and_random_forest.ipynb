{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdeba1da",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forests - Machine Learning with Python\n",
    "\n",
    "![](https://i.imgur.com/N8aIuRK.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec8395",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "This tutorial takes a practical and coding-focused approach. We'll learn how to use _decision trees_ and _random forests_ to solve a real-world problem from [Kaggle](https://kaggle.com/datasets):\n",
    "\n",
    "> **QUESTION**: The [Rain in Australia dataset](https://kaggle.com/jsphyg/weather-dataset-rattle-package) contains about 10 years of daily weather observations from numerous Australian weather stations. Here's a small sample from the dataset:\n",
    "> \n",
    "> ![](https://i.imgur.com/5QNJvir.png)\n",
    ">\n",
    "> As a data scientist at the Bureau of Meteorology, you are tasked with creating a fully-automated system that can use today's weather data for a given location to predict whether it will rain at the location tomorrow. \n",
    ">\n",
    ">\n",
    "> ![](https://i.imgur.com/KWfcpcO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628fe89",
   "metadata": {},
   "source": [
    "Let's install and import some required libraries before we begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0725804",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opendatasets pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4de403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',150)\n",
    "sns.set_style('darkgrid')\n",
    "matplotlib.rcParams['font.size']=14\n",
    "matplotlib.rcParams['figure.figsize']=(10,6)\n",
    "matplotlib.rcParams['figure.facecolor']='#00000000'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dda29d",
   "metadata": {},
   "source": [
    "## Downloading the Data\n",
    "\n",
    "The dataset is available at https://www.kaggle.com/jsphyg/weather-dataset-rattle-package .\n",
    "\n",
    "\n",
    "We'll use the [`opendatasets` library](https://github.com/JovianML/opendatasets) to download the data from Kaggle directly within Jupyter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "od.download('https://www.kaggle.com/jsphyg/weather-dataset-rattle-package')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5237a8",
   "metadata": {},
   "source": [
    "The dataset is downloaded and extracted to the folder `weather-dataset-rattle-package`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25331b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('weather-dataset-rattle-package')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99e4de",
   "metadata": {},
   "source": [
    "The file `weatherAUS.csv` contains the data. Let's load it into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eebadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df=pd.read_csv('weather-dataset-rattle-package/weatherAUS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af8d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad143c",
   "metadata": {},
   "source": [
    "Each row shows the measurements for a given date at a given location. The last column \"RainTomorrow\" contains the value to be predicted.\n",
    "\n",
    "Let's check the column types of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62872efa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's drop any rows where the value of the target column `RainTomorrow` in empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd23a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.dropna(subset=['RainTomorrow'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb91a44",
   "metadata": {},
   "source": [
    "## Preparing the Data for Training\n",
    "\n",
    "We'll perform the following steps to prepare the dataset for training:\n",
    "\n",
    "1. Create a train/test/validation split\n",
    "2. Identify input and target columns\n",
    "3. Identify numeric and categorical columns\n",
    "4. Impute (fill) missing numeric values\n",
    "5. Scale numeric values to the $(0, 1)$ range\n",
    "6. Encode categorical columns to one-hot vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f7435",
   "metadata": {},
   "source": [
    "### Training, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f3d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('No. of Rows per Year')\n",
    "sns.countplot(x=pd.to_datetime(raw_df.Date).dt.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9713e1",
   "metadata": {},
   "source": [
    "While working with chronological data, it's often a good idea to separate the training, validation and test sets with time, so that the model is trained on data from the past and evaluated on data from the future.\n",
    "\n",
    "We'll use the data till 2014 for the training set, data from 2015 for the validation set, and the data from 2016 & 2017 for the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f01a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "year=pd.to_datetime(raw_df.Date).dt.year\n",
    "train_df=raw_df[year<2015]\n",
    "val_df=raw_df[year==2015]\n",
    "test_df=raw_df[year>2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb6df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_df.shape: ',train_df.shape)\n",
    "print('val_df.shape: ',val_df.shape)\n",
    "print('test_df.shape',test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c1320e",
   "metadata": {},
   "source": [
    "### Input and Target Columns\n",
    "\n",
    "Let's identify the input and target columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols=list(train_df.columns)[1:-1]\n",
    "target_col='RainTomorrow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d55b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs=train_df[input_cols].copy()\n",
    "train_targets=train_df[target_col].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf95d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inputs=val_df[input_cols].copy()\n",
    "val_targets=val_df[target_col].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48745c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs=test_df[input_cols].copy()\n",
    "test_targets=test_df[target_col].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b935c",
   "metadata": {},
   "source": [
    "Let's also identify the numeric and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bce6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols=train_inputs.select_dtypes(include=np.number).columns.to_list()\n",
    "categorical_cols=train_inputs.select_dtypes('object').columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63181971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167cb84",
   "metadata": {},
   "source": [
    "### Imputing missing numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea39a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer=SimpleImputer(strategy='mean').fit(raw_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f10243",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs[numeric_cols]=imputer.transform(train_inputs[numeric_cols])\n",
    "val_inputs[numeric_cols]=imputer.transform(val_inputs[numeric_cols])\n",
    "test_inputs[numeric_cols]=imputer.transform(test_inputs[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a034761",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs[numeric_cols].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84179aa2",
   "metadata": {},
   "source": [
    "### Scaling Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dfb185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler().fit(raw_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6472c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs[numeric_cols]=scaler.transform(train_inputs[numeric_cols])\n",
    "val_inputs[numeric_cols]=scaler.transform(val_inputs[numeric_cols])\n",
    "test_inputs[numeric_cols]=scaler.transform(test_inputs[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4299e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_inputs.describe().loc[['min','max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83e48f",
   "metadata": {},
   "source": [
    "### Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=OneHotEncoder(sparse_output=False,handle_unknown='ignore').fit(raw_df[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_cols=list(encoder.get_feature_names_out(categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107fcc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1972e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs[encoded_cols]=encoder.transform(train_inputs[categorical_cols])\n",
    "val_inputs[encoded_cols]=encoder.transform(val_inputs[categorical_cols])\n",
    "test_inputs[encoded_cols]=encoder.transform(test_inputs[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00558596",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ed792",
   "metadata": {},
   "source": [
    "As a final step, let's drop the textual categorical columns, so that we're left with just numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b642473",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_inputs[numeric_cols+encoded_cols]\n",
    "X_val=val_inputs[numeric_cols+encoded_cols]\n",
    "X_test=test_inputs[numeric_cols+encoded_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4734f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af789b2",
   "metadata": {},
   "source": [
    "## Training and Visualizing Decision Trees\n",
    "\n",
    "A decision tree in general parlance represents a hierarchical series of binary decisions:\n",
    "\n",
    "<img src=\"https://i.imgur.com/qSH4lqz.png\" width=\"480\">\n",
    "\n",
    "A decision tree in machine learning works in exactly the same way, and except that we let the computer figure out the optimal structure & hierarchy of decisions, instead of coming up with criteria manually.m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47471c0",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We can use `DecisionTreeClassifier` from `sklearn.tree` to train a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fefabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9dc894",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(X_train,train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c86272",
   "metadata": {},
   "source": [
    "An optimal decision tree has now been created using the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f62efe6",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Let's evaluate the decision tree using the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f7a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a75ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds=model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3bb9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(train_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ce51a7",
   "metadata": {},
   "source": [
    "The decision tree also returns probabilities for each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probs=model.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da33e96f",
   "metadata": {},
   "source": [
    "Seems like the decision tree is quite confident about its predictions.\n",
    "\n",
    "Let's check the accuracy of its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e85f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(train_targets,train_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34408b92",
   "metadata": {},
   "source": [
    "The training set accuracy is close to 100%! But we can't rely solely on the training set accuracy, we must evaluate the model on the validation set too. \n",
    "\n",
    "We can make predictions and compute accuracy in one step using `model.score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b144265",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_val,val_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a79a7",
   "metadata": {},
   "source": [
    "Although the training accuracy is 100%, the accuracy on the validation set is just about 79%, which is only marginally better then always predicting \"No\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_targets.value_counts()/len(val_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd5394b",
   "metadata": {},
   "source": [
    "It appears that the model has learned the training examples perfect, and doesn't generalize well to previously unseen examples. This phenomenon is called \"overfitting\", and reducing overfitting is one of the most important parts of any machine learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3c9f5",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "We can visualize the decision tree _learned_ from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7fa385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree,export_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,20))\n",
    "plot_tree(model,feature_names=X_train.columns,max_depth=2,filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad992563",
   "metadata": {},
   "source": [
    "Can you see how the model classifies a given input as a series of decisions? The tree is truncated here, but following any path from the root node down to a leaf will result in \"Yes\" or \"No\". Do you see how a decision tree differs from a logistic regression model?\n",
    "\n",
    "\n",
    "**How a Decision Tree is Created**\n",
    "\n",
    "Note the `gini` value in each box. This is the loss function used by the decision tree to decide which column should be used for splitting the data, and at what point the column should be split. A lower Gini index indicates a better split. A perfect split (only one class on each side) has a Gini index of 0. \n",
    "\n",
    "For a mathematical discussion of the Gini Index, watch this video: https://www.youtube.com/watch?v=-W0DnxQK1Eo . It has the following formula:\n",
    "\n",
    "<img src=\"https://i.imgur.com/CSC0gAo.png\" width=\"240\">\n",
    "\n",
    "Conceptually speaking, while training the models evaluates all possible splits across all possible columns and picks the best one. Then, it recursively performs an optimal split for the two portions. In practice, however, it's very inefficient to check all possible splits, so the model uses a heuristic (predefined strategy) combined with some randomization.\n",
    "\n",
    "The iterative approach of the machine learning workflow in the case of a decision tree involves growing the tree layer-by-layer:\n",
    "\n",
    "<img src=\"https://www.deepnetts.com/blog/wp-content/uploads/2019/02/SupervisedLearning.png\" width=\"480\">\n",
    "\n",
    "\n",
    "Let's check the depth of the tree that was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66705daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bd8196",
   "metadata": {},
   "source": [
    "We can also display the tree as text, which can be easier to follow for deeper trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504368cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_text=export_text(model,max_depth=10,feature_names=list(X_train.columns))\n",
    "tree_text[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47f9f9",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Based on the gini index computations, a decision tree assigns an \"importance\" value to each feature. These values can be used to interpret the results given by a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41641402",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546eae2b",
   "metadata": {},
   "source": [
    "Let's turn this into a dataframe and visualize the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df=pd.DataFrame({\n",
    "    'feature':X_train.columns,\n",
    "    'importance':model.feature_importances_\n",
    "}).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf465da",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Feature Importance')\n",
    "sns.barplot(data=importance_df.head(10),x='importance',y='feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a5dae5",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning and Overfitting\n",
    "\n",
    "As we saw in the previous section, our decision tree classifier memorized all training examples, leading to a 100% training accuracy, while the validation accuracy was only marginally better than a dumb baseline model. This phenomenon is called overfitting, and in this section, we'll look at some strategies for reducing overfitting.\n",
    "\n",
    "The `DecisionTreeClassifier` accepts several arguments, some of which can be modified to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fada65",
   "metadata": {},
   "source": [
    "These arguments are called hyperparameters because they must be configured manually (as opposed to the parameters within the model which are _learned_ from the data. We'll explore a couple of hyperparameters:\n",
    "\n",
    "- `max_depth`\n",
    "- `max_leaf_nodes`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0885e",
   "metadata": {},
   "source": [
    "### `max_depth`\n",
    "\n",
    "By reducing the maximum depth of the decision tree, we can prevent the tree from memorizing all training examples, which may lead to better generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a0b315",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=DecisionTreeClassifier(max_depth=3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61d5eb",
   "metadata": {},
   "source": [
    "We can compute the accuracy of the model on the training and validation sets using `model.score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7468d55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6fe31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_val,val_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64b143",
   "metadata": {},
   "source": [
    "Great, while the training accuracy of the model has gone down, the validation accuracy of the model has increased significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37407cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,20))\n",
    "plot_tree(model,feature_names=X_train.columns,filled=True,rounded=True,class_names=model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbef0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(export_text(model,feature_names=list(X_train.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed13fc",
   "metadata": {},
   "source": [
    "Let's experiment with different depths using a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d513df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_depth_error(md):\n",
    "    model=DecisionTreeClassifier(max_depth=md,random_state=42)\n",
    "    model.fit(X_train,train_targets)\n",
    "    train_acc=1-model.score(X_train,train_targets)\n",
    "    val_acc=1-model.score(X_val,val_targets)\n",
    "    return {'Max Depth':md,'Training Error': train_acc,'Validation Error':val_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "errors_df=pd.DataFrame([max_depth_error(md) for md in range(1,21)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5977846",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d13756",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(errors_df['Max Depth'],errors_df['Training Error'])\n",
    "plt.plot(errors_df['Max Depth'],errors_df['Validation Error'])\n",
    "plt.title('Training vs Validation Error')\n",
    "plt.xticks(range(1,21,2))\n",
    "plt.xlabel('Max. Depth')\n",
    "plt.ylabel('Prediction Error (1-Accuracy)')\n",
    "plt.legend(['Training','Validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d7142",
   "metadata": {},
   "source": [
    "\n",
    "This is a common pattern you'll see with all machine learning algorithms:\n",
    "\n",
    "<img src=\"https://i.imgur.com/EJCrSZw.png\" width=\"480\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c182ebd",
   "metadata": {},
   "source": [
    "You'll often need to tune hyperparameters carefully to find the optimal fit. In the above case, it appears that a maximum depth of 7 results in the lowest validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb52e8",
   "metadata": {},
   "source": [
    "### `max_leaf_nodes`\n",
    "\n",
    "Another way to control the size of complexity of a decision tree is to limit the number of leaf nodes. This allows branches of the tree to have varying depths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=DecisionTreeClassifier(max_leaf_nodes=128,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf4031",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_val,val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e13e52f",
   "metadata": {},
   "source": [
    "Notice that the model was able to achieve a greater depth of 12 for certain paths while keeping other paths shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed00f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text=export_text(model,feature_names=list(X_train.columns))\n",
    "print(model_text[:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31ce3e7",
   "metadata": {},
   "source": [
    "## Training a Random Forest\n",
    "\n",
    "While tuning the hyperparameters of a single decision tree may lead to some improvements, a much more effective strategy is to combine the results of several decision trees trained with slightly different parameters. This is called a random forest. \n",
    "\n",
    "The key idea here is that each decision tree in the forest will make different kinds of errors, and upon averaging, many of their errors will cancel out. This idea is also known as the \"wisdom of the crowd\" in common parlance:\n",
    "\n",
    "<img src=\"https://i.imgur.com/4Dg0XK4.png\" width=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a5fe00",
   "metadata": {},
   "source": [
    "A random forest works by averaging/combining the results of several decision trees:\n",
    "\n",
    "<img src=\"https://1.bp.blogspot.com/-Ax59WK4DE8w/YK6o9bt_9jI/AAAAAAAAEQA/9KbBf9cdL6kOFkJnU39aUn4m8ydThPenwCLcBGAsYHQ/s0/Random%2BForest%2B03.gif\" width=\"640\">\n",
    "\n",
    "\n",
    "We'll use the `RandomForestClassifier` class from `sklearn.ensemble`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c72fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestClassifier(n_jobs=-1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd088741",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(X_train,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3846b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5cb5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_val,val_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04be007",
   "metadata": {},
   "source": [
    "Once again, the training accuracy is 100%, but this time the validation accuracy is much better. In fact, it is better than the best single decision tree we had trained so far. Do you see the power of random forests?\n",
    "\n",
    "This general technique of combining the results of many models is called \"ensembling\", it works because most errors of individual models cancel out on averaging. Here's what it looks like visually:\n",
    "\n",
    "<img src=\"https://i.imgur.com/qJo8D8b.png\" width=\"480\">\n",
    "\n",
    "\n",
    "We can also look at the probabilities for the predictions. The probability of a class is simply the fraction of trees which that predicted the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probs=model.predict_proba(X_train)\n",
    "train_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6ecfa",
   "metadata": {},
   "source": [
    "We can can access individual decision trees using `model.estimators_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb9e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,20))\n",
    "plot_tree(model.estimators_[0],max_depth=2,feature_names=X_train.columns,filled=True,rounded=True,class_names=model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa6fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,20))\n",
    "plot_tree(model.estimators_[15],max_depth=2,feature_names=X_train.columns,filled=True,rounded=True,class_names=model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.estimators_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4144e87",
   "metadata": {},
   "source": [
    "Just like decision tree, random forests also assign an \"importance\" to each feature, by combining the importance values from individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df=pd.DataFrame({\n",
    "    'feature':X_train.columns,\n",
    "    'importance':model.feature_importances_\n",
    "}).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87101aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Feature Importance')\n",
    "sns.barplot(data=importance_df.head(10),x='importance',y='feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7905c",
   "metadata": {},
   "source": [
    "Notice that the distribution is a lot less skewed than that for a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45596d60",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Random Forests\n",
    "\n",
    "Just like decision trees, random forests also have several hyperparameters. In fact many of these hyperparameters are applied to the underlying decision trees. \n",
    "\n",
    "Let's study some the hyperparameters for random forests. You can learn more about them here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa2e19",
   "metadata": {},
   "source": [
    "Let's create a base model with which we can compare models with tuned hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=RandomForestClassifier(random_state=42,n_jobs=-1).fit(X_train,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eed7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_acc=base_model.score(X_train,train_targets)\n",
    "base_val_acc=base_model.score(X_val,val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1acc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs=base_train_acc,base_val_acc\n",
    "base_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9eebea",
   "metadata": {},
   "source": [
    "### `n_estimators`\n",
    "\n",
    "This controls the number of decision trees in the random forest. The default value is 100. For larger datasets, it helps to have a greater number of estimators. As a general rule, try to have as few estimators as needed. \n",
    "\n",
    "\n",
    "**10 estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac9cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestClassifier(random_state=42,n_jobs=-1,n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58fcb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train,train_targets),model.score(X_val,val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1cd26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de986a0",
   "metadata": {},
   "source": [
    "**500 estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94bdb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestClassifier(random_state=42,n_jobs=-1,n_estimators=500)\n",
    "model.fit(X_train,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f716ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c636d",
   "metadata": {},
   "source": [
    "### `max_features`\n",
    "\n",
    "Instead of picking all features for every split, we can specify only a fraction of features to be chosen randomly.\n",
    "\n",
    "<img src=\"https://i.imgur.com/FXGWMDY.png\" width=\"720\">\n",
    "\n",
    "Notice that the default value `auto` causes only $\\sqrt{n}$ out of total features ( $n$ ) to be chosen randomly at each split. This is the reason each decision tree is in the forest is different. While it may seem counterintuitive, choosing all features for every split of every tree will lead to identical trees, so the random forest will not generalize well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c20a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_params(**params):\n",
    "    model=RandomForestClassifier(random_state=42,n_jobs=-1,**params).fit(X_train,train_targets)\n",
    "    return model.score(X_train,train_targets),model.score(X_val,val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b6b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_features='log2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4948ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437bac17",
   "metadata": {},
   "source": [
    "### `max_depth` and `max_leaf_nodes`\n",
    "\n",
    "These arguments are passed directly to each decision tree, and control the maximum depth and max. no leaf nodes of each tree respectively. By default, no maximum depth is specified, which is why each tree has a training accuracy of 100%. You can specify a `max_depth` to reduce overfitting.\n",
    "\n",
    "<img src=\"https://i.imgur.com/EJCrSZw.png\" width=\"480\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215971f1",
   "metadata": {},
   "source": [
    "Let's test a few values of `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46392368",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9758fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_depth=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae76b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_leaf_nodes=2**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_leaf_nodes=2**20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce83d31",
   "metadata": {},
   "source": [
    "The optimal values of `max_depth` and `max_leaf_nodes` lies somewhere between 0 and unbounded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026fed3d",
   "metadata": {},
   "source": [
    "### `min_samples_split` and `min_samples_leaf`\n",
    "\n",
    "By default, the decision tree classifier tries to split every node that has 2 or more. You can increase the values of these arguments to change this behavior and reduce overfitting, especially for very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f8dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(min_samples_split=3, min_samples_leaf=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(min_samples_split=100, min_samples_leaf=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0a0ff",
   "metadata": {},
   "source": [
    "### `min_impurity_decrease`\n",
    "\n",
    "This argument is used to control the threshold for splitting nodes. A node will be split if this split induces a decrease of the impurity (Gini index) greater than or equal to this value. It's default value is 0, and you can increase it to reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed838c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(min_impurity_decrease=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29229c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(min_impurity_decrease=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e524d8c8",
   "metadata": {},
   "source": [
    "### `bootstrap`, `max_samples` \n",
    "\n",
    "By default, a random forest doesn't use the entire dataset for training each decision tree. Instead it applies a technique called bootstrapping. For each tree, rows from the dataset are picked one by one randomly, with replacement i.e. some rows may not show up at all, while some rows may show up multiple times.\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/W8UGaEA.png\" width=\"640\">\n",
    "\n",
    "Bootstrapping helps the random forest generalize better, because each decision tree only sees a fraction of th training set, and some rows randomly get higher weightage than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(bootstrap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8611d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba552280",
   "metadata": {},
   "source": [
    "When bootstrapping is enabled, you can also control the number or fraction of rows to be considered for each bootstrap using `max_samples`. This can further generalize the model.\n",
    "\n",
    "<img src=\"https://i.imgur.com/rsdrL1W.png\" width=\"640\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(max_samples=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d056c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc236a1",
   "metadata": {},
   "source": [
    "### `class_weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c3e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params(class_weight={'No':1,'Yes':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ed8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807c7f6",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "\n",
    "Let's train a random forest with customized hyperparameters based on our learnings. Of course, different hyperpraams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6822c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RandomForestClassifier(n_jobs=-1,\n",
    "                             random_state=42,\n",
    "                             n_estimators=500,\n",
    "                             max_features=7,\n",
    "                             max_depth=30,\n",
    "                             class_weight={'No':1,'Yes':1.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec77fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5676da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train,train_targets),model.score(X_val,val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c179014",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db74b5",
   "metadata": {},
   "source": [
    "We've increased the accuracy from 84.5% with a single decision tree to 85.6% with a well-tuned random forest. Depending on the dataset, you may or may not a see a significant improvement with hyperparameter tuning. This could be due to any of the following reasons:\n",
    "\n",
    "- There simply may not be enough signal in the dataset. Whether it will rain tomorrow is an inherently uncertain outcome, and we can't predict it accurac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daa55e4",
   "metadata": {},
   "source": [
    "Let's also compute the accuracy of our final model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e041b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad53b7",
   "metadata": {},
   "source": [
    "## Making Predictions on New Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd90ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_input(model,single_input):\n",
    "    input_df = pd.DataFrame([single_input])\n",
    "    input_df[numeric_cols] = imputer.transform(input_df[numeric_cols])\n",
    "    input_df[numeric_cols] = scaler.transform(input_df[numeric_cols])\n",
    "    input_df[encoded_cols] = encoder.transform(input_df[categorical_cols])\n",
    "    X_input = input_df[numeric_cols + encoded_cols]\n",
    "    pred = model.predict(X_input)[0]\n",
    "    prob = model.predict_proba(X_input)[0][list(model.classes_).index(pred)]\n",
    "    return pred, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input={\n",
    "    'Date':'2021-06-19',\n",
    "    'Location':'Launceston',\n",
    "    'MinTemp':23.2,\n",
    "    'MaxTemp':33.2,\n",
    "    'Rainfall':10.2,\n",
    "    'Evaporation':4.2,\n",
    "    'Sunshine':np.nan,\n",
    "    'WindGustDir':'NNW',\n",
    "    'WindGustSpeed':52.0,\n",
    "    'WindDir9am':'NW',\n",
    "    'WindDir3pm':'NNE',\n",
    "    'WindSpeed9am':13.0,\n",
    "    'WindSpeed3pm':20.0,\n",
    "    'Humidity9am':89.0,\n",
    "    'Humidity3pm':58.0,\n",
    "    'Pressure9am':1004.8,\n",
    "    'Pressure3pm':1001.5,\n",
    "    'Cloud9am':8.0,\n",
    "    'Cloud3pm':5.0,\n",
    "    'Temp9am':25.7,\n",
    "    'Temp3pm':33.0,\n",
    "    'RainToday':'Yes'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7712ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input(model,new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.Location.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca23dc",
   "metadata": {},
   "source": [
    "### Saving and Loading Training Models\n",
    "\n",
    "We can use the `joblib` module to save and load Python objects on the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da82a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "aussie_rain={\n",
    "    'model':model,\n",
    "    'imputer':imputer,\n",
    "    'scaler':scaler,\n",
    "    'encoder':encoder,\n",
    "    'input_cols':input_cols,\n",
    "    'target_col':target_col,\n",
    "    'numeric_cols':numeric_cols,\n",
    "    'categorical_cols':categorical_cols,\n",
    "    'encoded_cols':encoded_cols,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(aussie_rain,'aussie_rain.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def65bc1",
   "metadata": {},
   "source": [
    "The object can be loaded back using `joblib.load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7963f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "aussie_rain2=joblib.load('aussie_rain.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b8c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds2=aussie_rain2['model'].predict(X_test)\n",
    "accuracy_score(test_targets,test_preds2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jskre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
